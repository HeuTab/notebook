# 动手学深度学习

## 添加jupyternote book环境

```
在环境里:
conda install ipykernel
python -m ipykernel install --name XXXX
```

## 数据操作+数据预处理

### 广播机制

```python
a = torch.arange(3).reshape((3,1))
tensor([[0],
        [1],
        [2]])
b = torch.arange(2).reshape((1,2))
tensor([[0, 1]])
a + b
输出：
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

### 数据预处理

```python
'''
	创建csv文件
'''
import os
os.makedirs(os.path.join('..','data'),exist_ok=True)
data_file = os.path.join('..','data','house_tiny.csv')
with open(data_file,'w') as f:
    f.write('NumRooms,Alley,Price\n')#列名
    f.write('NA,Pave,127500\n')#每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
'''
	读取csv文件
'''
import pandas as pd
data = pd.read_csv(data_file)
print(data)
inputs,outputs = data.iloc[:,0:2],data.iloc[:,2]	#进行分列读取数据
inputs = inputs.fillna(inputs.mean())//inputs.fillna表示将nan的值填充为什么，inputs.mean()表示取得数据的平均数
inputs
```

## 线性代数

### 矩阵的转置

```
A = torch.arange(20).reshape(5,4)
A.T
tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
```

### 矩阵的克隆

```
B = A.clone()
```

### 指定求和汇总张量的轴

```python
A_sum_axis0 = A.sum(axis=0)
A.sum(axis=[0,1])

a = torch.ones(2,5,4)
a.sum(axis=1).shape
#输出 torch.Size([2, 4])
a.sum(axis=1,keepdims=True).shape
#输出 torch.Size([2, 1, 4])
#可以理解为对哪一维求和就是消除哪一维
```

## 矩阵的计算

后面性能的优化，都是针对矩阵求导的优化



深度神经网络耗费GPU的一个原因就是：求梯度的时候，需要存储正向计算的中间结果

## 查看张量存储在哪

```python
x.device

net = nn.Sepuential(nn.Linear(3,1))
net = net.to(device=try_gpu())


#确认模型参数存储在同一个GPU上
net[0].weight.data.device
```

```python
def try_gpu(i = 0):
    '''如果存在,则返回gpu(i),否则返回cpu()。'''
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():
    '''返回所有可用的gpu,如果没有gpu,则返回[cpu(),]。'''
    devices = [
        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())
    ]
    return devices if devices else [torch.device('cpu')]

```

